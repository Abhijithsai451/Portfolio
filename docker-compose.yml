version: '3.8'

services:
  # web Service (web-facing API gateway, proxies to chat_service)
  web_service:
    build:
      context: . # Build context is the project root
      dockerfile: web_service/Dockerfile
    ports:
      - "8000:8000"
    environment:

      ALLOWED_HOSTS: "localhost,127.0.0.1"
      REDIS_URL: "redis://redis:6379" # Connects to the 'redis' service defined below
      CHAT_SERVICE_URL: "http://chat_service:8001"
    # Docker Compose can load environment variables from a .env file at the project root
    env_file:
      - .env
    volumes:
      - .:/app
    depends_on:
      - chat_service
      - redis

  # Chat Service (AI/ML logic, ChromaDB, Ollama interaction)
  chat_service:
    build:
      context: . # Build context is the project root
      dockerfile: chat_service/Dockerfile
    environment:
      OLLAMA_BASE_URL: "http://host.docker.internal:11434" # Adjust this based on your Ollama setup
      EMBEDDING_MODEL: "nomic-embed-text"
      CHAT_MODEL: "llama3.1"
      KNOWLEDGE_BASE_FILE: "chat_service/knowledge_base.txt"
      USE_VECTOR_DB: "true"
      REDIS_URL: "redis://redis:6379"
      CACHE_TTL: "300"
    env_file:
      - .env
    volumes:
      - chat_service_chroma_db_data:/app/chroma_db
      - chat_service_st_cache:/app/.cache/torch/sentence_transformers
      - .:/app
    depends_on:
      - redis

  # Redis service for caching and session management
  redis:
    image: "redis/redis-stack-server:latest" # A full-featured Redis with GUI (RedisInsight)
    ports:
      - "6379:6379" # Map host port 6379 to container port 6379
      - "8002:8001" # Optional: Map RedisInsight GUI (if you want to access it from host)
    volumes:
      - redis_data:/data # Persist Redis data
    command: redis-stack-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD}

volumes:
  # Define named volumes for persistent data
  chat_service_chroma_db_data:
  chat_service_st_cache:
  redis_data: